# -*- coding: utf-8 -*-
"""HW_Unsupervised_Clustering<Syahrul Ilham Permana>.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FW6tgHD-1Qs-9X41qrc7lXEZV8T8Q-TD

# **0. Load Dataset**
"""

!pip install pandasql

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings

# %matplotlib inline

print('numpy version : ',np.__version__)
print('pandas version : ',pd.__version__)
print('seaborn version : ',sns.__version__)

from mpl_toolkits.mplot3d import Axes3D

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/flight.csv')
data.head()

"""# **1. Data Understanding**

## **1.1 Statistical Summary**

### **1.1.1 Tipe Data**
"""

data.info()

"""- Feature seharusnya memiliki tipe data *datetime* : `FFP_DATE`, `FIRST_FLIGHT_DATE`, `LOAD_TIME`, `LAST_FLIGHT_DATE`
- Feature `AGE` sebaiknya *integer*
"""

# jumlah record dan feature
print('Total Record :', data.shape[0])
print('Total Features :', data.shape[1])

# sampling dataset
data.sample(3)

"""### **1.1.2 Missing Value**"""

# cek data yang hilang
counts = data.isna().sum().sort_values(ascending=False)

# persentase data yang hilang
percents = round(data.isna().mean() * 100, 2).sort_values(ascending=False)
null = pd.concat([counts, percents], axis=1, keys=["null_counts", "percents(%)"])
null

# Persentase record/rows yang mengandung missing value
data_nan = data[data.isna().any(axis=1)]
print(f'Persentase missing value pada dataset : {round(len(data_nan)/len(data)*100, 2)}%')

"""- Terdapat 5 feature yang memiliki missing value yaitu : `WORK_PROVINCE`, `WORK_CITY`, `SUM_YR_1`, `AGE`, `SUM_YR_2`, `WORK_COUNTRY`
- Dataset memiliki 7.51% missing value (masih dibawah 10%)

### **1.1.3 Duplikat Data**
"""

# cek duplikat
data.duplicated().sum()

"""Tidak terdapat duplikat di dataset

##Pengelompokan Data Berdasarkan Tipe Data
"""

# cek feature yang memiliki nilai numerik
numeric = data_nan.loc[:, (data_nan.dtypes == int) | (data_nan.dtypes == float)].columns.tolist()
print(numeric)
print(f'Jumlah feature numerikal : {len(numeric)}')

# cek data yang memiliki nilai kategorik
category = data_nan.loc[:, (data_nan.dtypes == object)].columns.tolist()
print(category)
print(f'Jumlah feature kategorikal : {len(category)}')

# split data berdasarkan kategori
numeric = ['MEMBER_NO', 'AGE', 'FFP_TIER', 'FLIGHT_COUNT', 'BP_SUM', 'SUM_YR_1', 'SUM_YR_2', 'SEG_KM_SUM', 'LAST_TO_END', 'AVG_INTERVAL', 'MAX_INTERVAL', 'EXCHANGE_COUNT',
       'avg_discount', 'Points_Sum', 'Point_NotFlight']
category = ['GENDER', 'WORK_CITY', 'WORK_PROVINCE', 'WORK_COUNTRY', 'FFP_DATE', 'FIRST_FLIGHT_DATE', 'LOAD_TIME', 'LAST_FLIGHT_DATE']

"""### **1.1.5 Statistika Deskriptif**"""

# analisis deskriptif data numerikal
data[numeric].describe().T

"""**Observasi :**
- Terdapat nilai 0 yaitu `BP_SUM`, `SUM_YR_1`, `SUM_YR_2`, `AVG_INTERVAL`, `MAX_INTERVAL`, `EXCHANGE_COUNT`, `avg_discount`, `Points_Sum`, `Point_NotFlight`.
- `MEMBER_NO`, `FFP_TIER` dan `avg_discount` memiliki nilai mean yang hampir sama dengan median yang menandakan fitur tersebut cenderung mendekati distribusi normal.
"""

# analisis deskriptif data kategorikal
data[category].describe().T

# Jumlah category
for col in category:
    print(f'''Value count kolom {col}:''')
    print('-' * 50)
    print(data[col].value_counts())
    print()

"""### **1.1.6 Summary**

**Hasil :**
- Dataset memiliki 15 feature numerikal dan 8 feature kategorikal.
- Rekomendasi tipe data feature `FFP_DATE`, `FIRST_FLIGHT_DATE`, `LOAD_TIME`, `LAST_FLIGHT_DATE` sebaiknya menggunakan datetime dan `AGE` menggunakan integer.
- Dataset memiliki missing value dibawah 10%
- Tidak terdapat record yang duplikat.
- Nilai 0 pada feature `EXCHANGE_COUNT`, `avg_discount`, `Points_Sum`, `Point_NotFlight` kemungkinan mempresentasikan user tidak mendapat atau melakukan action dari feature tersebut.
- Nilai 0 pada feature `SUM_YR_1`, `SUM_YR_2`,  ` AVG_INTERVAL`, `MAX_INTERVAL` kemungkinan user tidak melakukan penerbangan, hal tersebut dapat dianalisis lebih lanjut.
- Mayoritas user adalah Laki-laki berasal dari China, provinsi Guangdong, kota Guangzhou

## **1.2 Univariate Analysis**

### **1.2.1 Feature Numerikal**
"""

# cek distribusi dan outlier menggunakan boxplot
plt.figure(figsize=(12, 6))
for i in range(0, len(numeric)):
    plt.subplot(3, 5, i+1)
    sns.boxplot(y=data[numeric[i]], color='#91bfdb', orient='v')
    plt.tight_layout()

"""### **1.2.2 Feature Kategorikal**"""

# countplot feature kategorikal
plt.figure(figsize=(17, 6))
for i in range(0, len(category)):
    plt.subplot(2, 4, i+1)
    sns.countplot(x=data[category[i]], color='#91bfdb', orient='v')
    plt.tight_layout()

"""Tidak terlalu banyak insight yang didapatkan dalam `WORK_CITY`, `WORK_PROVINCE`, `WORK_COUNTRY` maka diputuskan untuk membuat visualisasi top 10 untuk kolom tersebut

"""

# membuat grafik dengan format 3 rows dengan 1 kolom
fig, ax = plt.subplots(nrows=3,ncols=1,figsize=(10,6))

# work_city
work_city_top10 = data['WORK_CITY'].value_counts().head(10)
sns.barplot(x=work_city_top10.index,y=work_city_top10.values,ax=ax[0], palette=('Paired'))
ax[0].set_xlabel('WORK_CITY')
ax[0].set_ylabel('count')

# work_province
work_province_top10 = data['WORK_PROVINCE'].value_counts().head(10)
sns.barplot(x=work_province_top10.index,y=work_province_top10.values,ax=ax[1], palette=('RdYlBu'))
ax[1].set_xlabel('WORK_PROVINCE')
ax[1].set_ylabel('count')

# work_country
work_country_top10 = data['WORK_COUNTRY'].value_counts().head(10)
sns.barplot(x=work_country_top10.index,y=work_country_top10.values,ax=ax[2], palette=('PRGn'))
ax[2].set_xlabel('WORK_COUNTRY')
ax[2].set_ylabel('count')

"""Visualisasi `FFP_DATE`, `FIRST_FLIGHT_DATE` dan `LAST_FLIGHT_DATE` per tahun"""

# membuat grafik dengan format 2 rows dengan 1 kolom
fig, ax = plt.subplots(nrows=2,ncols=1,figsize=(10,7))

# mengubah tipe object ke datetime

# FFP
FFP = pd.to_datetime(data['FFP_DATE']).dt.year
sns.countplot(x=FFP,ax=ax[0], palette=('RdYlBu'))
ax[0].tick_params(axis='x')
ax[0].set_xlabel('FFP DATE YEAR')

# FFD
FFD = pd.to_datetime(data['FIRST_FLIGHT_DATE']).dt.year
sns.countplot(x=FFD, ax=ax[1], palette=('RdYlBu'))
ax[1].tick_params(axis='x')
ax[1].set_xlabel('FIRST FLIGHT DATE YEAR')

"""Pada feature `LAST_FLIGHT_DATE` ditemukan nilai tanggal 2014/2/29 yang tidak masuk akal karena 2014 bukanlah tahun kabisat."""

# mengcopy dataset untuk analisis sementara
data_last = data.copy()

# drop data yang memilik tanggal 2014/2/29
data_last.drop(data_last[data_last.LAST_FLIGHT_DATE.str.contains('2014/2/29')].index, inplace = True)

# membuat visualisasi
fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(8,3))
# mengubah tipe object ke datetime
LFD = pd.to_datetime(data_last['LAST_FLIGHT_DATE']).dt.year
# visualisasi
sns.countplot(x=LFD, palette='RdYlBu')
ax.set_xlabel('LAST FLIGHT DATE YEAR')

"""### **1.2.3 Summary**

**Hasil :**
- Mayoritas feature memiliki outlier, kecuali MEMBER_NO dan FFP_TIER
- Moyaritas feature memiliki distribusi yang positively skew, kecuali pada MEMBER_NO dan avg_discount cenderung mendekati normal.
- Mayoritas user adalah laki-laki
- Mayoritas feature kategorikal memiliki nilai unik
- LOAD_TIME memiliki 1 nilai yaitu 2014-03-31
- Mayoritas user yang bergabung menjadi membership pada tahun 2012
- Terdapat nilai tahun yang sangat jauh pada featur `FIRST FLIGHT DATE` yaitu 1905
- Pada feature `LAST_FLIGHT_DATE` terdapat nilai tanggal yang tidak masuk akal yaitu tanggal 2014/2/29, tahun tersebut bukan tahun kabisat sehingga tidak ada tanggal 29 di bulan Februari.

## **1.3 Multivariate Analysis**

### **1.3.1 Korelasi Antar Feature**
"""

# heatmap korelasi antar feature
plt.figure(figsize=(9, 9))
sns.heatmap(data.corr(), cmap='RdYlBu', annot=True, fmt='.2f')

"""### **1.3.2 Summary**

Feature yang memiliki korelasi tinggi :
- Antara feature-feature `FLIGHT_COUNT`, `BP_SUM`, `SUM_YR_1`, `SUM_YR_2`, `SEG_KM_SUM`, `Points_Sum`
- `AVG_INTERVAL` dengan `MAX_INTERVAL`
- Feature  `FLIGHT_COUNT`, `BP_SUM`,`SUM_YR_1`, `SUM_YR_2`, `SEG_KM_SUM` saling memiliki kaitan. Semakin sering pelanggan terbang, semakin jauh jarak penerbangan yang ditempuh dan semakin banyak pendapatan tarif.

# **2. Feature Enginering**

## **2.1 Data Cleaning**

### **2.1.1 Fixing Datatype**

Sebelum memperbaiki tipe dataset, diputuskan untuk menghapus feature-feature yang tidak relate dan memiliki banyak nilai unik yaitu `MEMBER_NO`, `WORK_CITY`, `WORK_PROVINCE`, `WORK_COUNTRY`, `AGE`, `GENDER`.
"""

# drop kolom
data_pre = data.drop(columns=['MEMBER_NO', 'WORK_CITY', 'WORK_PROVINCE', 'WORK_COUNTRY', 'AGE', 'GENDER']).copy()

"""Menghapus tanggal 2014/2/29 yang ditemukan pada `LAST_FLIGHT_DATE`."""

# menghapus record
data_pre.drop(data_pre[data_pre.LAST_FLIGHT_DATE.str.contains('2014/2/29')].index, inplace = True)

"""Mengubah tipe yang memiliki feature tanggal dengan datetime."""

# feature yang meiliki type datetime
date_feature = ['FFP_DATE', 'FIRST_FLIGHT_DATE', 'LOAD_TIME','LAST_FLIGHT_DATE']

# inisialisasi
for col in date_feature:
  data_pre[col] = pd.to_datetime(data_pre[col], errors='coerce')

data_pre.info()

"""### **2.1.2 Handling Missing Value**"""

data_pre.isna().sum()

# Persentase record/rows yang mengandung missing value
data_nan = data_pre[data_pre.isna().any(axis=1)]
print(f'Persentase missing value pada dataset : {round(len(data_nan)/len(data_pre)*100, 2)}%')

"""Dikarenakan missing value hanya sebesar 1.1%, diputuskan untuk menghapusnya karena tidak terlalu signifikan mempengaruhi hasil dalam pemodelan."""

# Menghapus baris data dari missing value SUM_YR_1, SUM_YR_2
data_pre = data_pre.dropna()

# cek missing value
data_pre.isna().sum()

"""Membersihkan data sesuai dengan prosedur standar untuk dataset penerbangan (Tao, 2020)
- Record di mana harga tiket (SUM_YR_1,SUM_YR_2) berisi nilai 0, dan diskon rata-rata (avg_discount) bukan nol, dan dimana total jarak tempuh (SEG_KM_SUM) lebih dari nol.
- Hal ini diasumsikan bahwa user tidak memiliki riwayat perjalanan
"""

# drop kolom
data_pre.drop(data_pre[(data_pre['SUM_YR_1'] == 0) & (data_pre['SUM_YR_2'] == 0) & (data_pre['avg_discount'] == 0) & (data_pre['SEG_KM_SUM'] > 0)].index, inplace = True)

data_pre.info()

"""## **2.2 Feature Selection**

Menggunakan LRFMC model. Penjelasan LRFMC menurut Tao (2020) adalah sebagai berikut:
- L (LOYALTY) : Lama waktu user menjadi membership (dalam bulan) dalam diketahui dari selisih dari waktu observasi dengan waktu menjadi menjadi membership.
- R (RECENCY) : Jumlah bulan sejak penerbangan terakhir anggota dari akhir waktu observasi.
- F (FREQUENCY) : Total berapa kali user telah terbang selama periode observasi.
- M (MONETERY) : Miles atau jarak yang terakumulasi selama waktu observasi user.
- C (DISCOUNT) : Nilai rata-rata discount factor yang digunakan user selama periode observasi.

Maka feature yang diambil :
- FPP_DATE (Frequent Flyer Program Join Date)
- LOAD_TIME (Tanggal Data Diambil)
- FLIGHT_COUNT (Jumlah Penerbangan Customer)
- AVG_DISCOUNT (Rata-rata Discount yang Diadapat Customer)
- SEG_KM_SUM (Total Jarak (km) Penerbangan yang Sudah Dilakukan)
- LAS_TO_END (Jarak Waktu Penerbangan Terakhir ke Pesanan Penerbangan Paling Terakhir)
"""

# select feature
data_feats = data_pre[['LOAD_TIME', 'FFP_DATE', 'LAST_TO_END', 'FLIGHT_COUNT','SEG_KM_SUM','avg_discount']]
data_feats['TIME_MONTH'] = ((data_pre['LOAD_TIME'] - data_pre['FFP_DATE']).dt.days/30).astype(int)
data_feats = data_feats[['TIME_MONTH', 'LAST_TO_END', 'FLIGHT_COUNT','SEG_KM_SUM','avg_discount']].copy()
data_feats.info()

data_feats.sample(3)

"""## **2.3 Handling Outliers**"""

lrfmc = ['TIME_MONTH', 'LAST_TO_END','FLIGHT_COUNT','SEG_KM_SUM','avg_discount']

plt.figure(figsize=(10, 4))
for i in range(0, len(lrfmc)):
    plt.subplot(1, 5, i+1)
    sns.boxplot(y=data_feats[lrfmc[i]], color='#91bfdb', orient='v')
    plt.tight_layout()

print(f'Jumlah baris sebelum memfilter outlier: {len(data_feats)}')

fil_ent = np.array([True] * len(data_feats))
for col in lrfmc:
    Q1 = data_feats[col].quantile(0.25)
    Q3 = data_feats[col].quantile(0.75)
    IQR = Q3 - Q1
    low_limit = Q1 - (IQR * 1.5)
    high_limit = Q3 + (IQR * 1.5)

    fil_ent = ((data_feats[col] >= low_limit) & (data_feats[col] <= high_limit)) & fil_ent

data_feats = data_feats[fil_ent].reset_index()
print('Jumlah baris setelah memfilter outlier', len(data_feats))

plt.figure(figsize=(10, 4))
for i in range(0, len(lrfmc)):
    plt.subplot(1, 5, i+1)
    sns.boxplot(y=data_feats[lrfmc[i]], color='#91bfdb', orient='v')
    plt.tight_layout()

data_feats=data_feats.drop(columns='index')

"""# **3. Modeling and Evaluation**

## **3.1 Scalling**
"""

# melihat statistical summary dataset
data_feats.describe()

# mengganti nama feature
data_feats.columns =['L', 'R', 'F', 'M', 'C']

# Standardisasi menggunakan StandardScaler
features = ['L', 'R', 'F', 'M', 'C']
X = data_feats[features].values

from sklearn.preprocessing import StandardScaler
X_std = StandardScaler().fit_transform(X)
df_std = pd.DataFrame(data = X_std, columns = features)

# melihat distribusi data yang telah dilakukan standardisasi
plt.figure(figsize=(15, 15))
for i in range(0, len(features)):
    plt.subplot(5, 5, i+1)
    sns.kdeplot(df_std[features[i]], color='#91bfdb')
    plt.xlabel(features[i])
    plt.tight_layout()

# melihat summary statistic
df_std.describe()

df_std=df_std.copy()

"""## **3.2 Best K**"""

# mencari nilai k optimal dengan parameter inertia
from sklearn.cluster import KMeans

inertia = []
k_values = range(2,11)

# fit model
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=123)
    kmeans.fit(df_std)
    inertia.append(kmeans.inertia_)

# Visualisasi grafik elbow method
from yellowbrick.cluster import KElbowVisualizer
plt.figure(figsize=(10, 5))

# plot grafik
plt.plot(k_values, inertia ,color='#91bfdb', linewidth= 2.5, marker='o', markerfacecolor='#fc8d59', markersize=10)
plt.title('Inertia Score Elbow', fontsize=14)
plt.xlabel('K value',fontsize=12)
plt.ylabel('Inertia',fontsize=1)

"""Berdasarkan grafik yang dihasilkan tidak terlalu terlihat penurunaan nilai dan patahan yang tajam, sehingga nilai k yang optimal tidak terlalu diketahui dengan pasti."""

# analisa lebih lanjut dengan persentase dari inertia
(pd.Series(inertia) - pd.Series(inertia).shift(-1)) / pd.Series(inertia) * 100

"""Dari selisih nilai yang dihasilkan, nilai k optimal diantara 2-4.

## **3.3 Silhouette Score**
"""

from yellowbrick.cluster import KElbowVisualizer

model = KMeans(random_state=123)
visualizer = KElbowVisualizer(model, k=(2,11), metric='silhouette', timings=True, locate_elbow=True)
visualizer.fit(df_std)
visualizer.show()

"""Dari grafik diatas diketahui k optimal = 2 atau 4

## **3.3 Clustering**
"""

from sklearn.cluster import KMeans

# fit model
kmeans = KMeans(n_clusters=4, random_state = 123)
kmeans.fit(df_std.values)
df_std['cluster'] = kmeans.labels_

df_std.head()

## melakukan PCA untuk melihat visualisasi
from sklearn.decomposition import PCA
pca = PCA(n_components=2)

pca.fit(df_std)
pcs = pca.transform(df_std)


df_pca = pd.DataFrame(data = pcs, columns = ['PC 1', 'PC 2'])
df_pca['cluster'] = df_std['cluster']
df_pca

# visualisasi hasi segmentasi
fig, ax = plt.subplots(figsize=(10,7))
sns.scatterplot(data=df_pca, x="PC 1", y="PC 2", hue="cluster", palette='RdYlBu')
plt.title('Customer Segmentation Based on LRFMC Model')

"""# **4. Insight**"""

# menambahkan cluster ke dataframe
data_feats['cluster'] = kmeans.labels_
display(data_feats.groupby('cluster').agg(['mean','median','min','max']))

# persentase customer setiap cluster
cluster_count = data_feats['cluster'].value_counts().reset_index()
cluster_count.columns = ['cluster', 'count']
cluster_count['percentage (%)'] = round((cluster_count['count']/len(df_std))*100,2)
cluster_count = cluster_count.sort_values(by=['cluster']).reset_index(drop=True)
cluster_count

#visualisasi persentase customer pada setiap cluster
fig, ax = plt.subplots(figsize=(8,4))

c = ["#d7191c","#fdae61","#ffffbf","#abd9e9","#2c7bb6"]
bars = plt.bar(x=cluster_count['cluster'], height= cluster_count['percentage (%)'],color=c)

for bar in bars:
  height = bar.get_height()
  label_x_pos = bar.get_x() + bar.get_width() / 2
  ax.text(label_x_pos, height, s=f'{height} %', ha='center',
  va='bottom')
plt.title('Percentage of Customer by Cluster', fontsize=16)
plt.xlabel('Cluster',fontsize=12)
plt.ylabel('Percentage',fontsize=12)

"""## **4.2 Segmentation Analysis**"""

# melihat rata-rata variabel LRFMC antar cluster
LRFMC= ['L','R','F','M','C']
def dist_list(lst):
    plt.figure(figsize=[len(lst)*4,3])
    i = 1
    for col in lst:
        ax = plt.subplot(1,len(lst),i)
        ax.vlines(data_feats[col].median(), ymin=0, ymax=3, color='grey', linestyle='--')
        g = data_feats.groupby('cluster')
        x = g[col].median().index
        y = g[col].median().values
        ax.barh(x,y, color=c)
        plt.title(col)
        i = i+1

dist_list(LRFMC)

"""Secara keseluruhan cluster-cluster ini terbentuk karena adanya penbedaan indikator model LRFMC. Berdasarkan hasil analisis dan visualisai dapat diketahui karakteristik customer pada setiap kelompok sebagai berikut:

<p style="text-align: center;">
Akumulasi Hasil Nilai LRFMC pada Setiap Cluster
</p>

|  Cluster  | High Value | Average Value | Low Value |
| :-------- | :--------: | :-----------: | :-------: |
| **Cluster 0** | F M C | L | R |
| **Cluster 1** | R C | L  | F M |
| **Cluster 2** | C | R F M | L |
| **Cluster 3** | L C | R F M |  |

**Interpretasi :** <br>

1. Cluster 0 - **Potential Loyalist - The Campions**
    - Kelompok customer yang memliki aktivitas penerbangan yang sangat tinggi, walupun rentang waktu untuk melakukan setiap penerbangan tidak terlalu jauh, kelompok ini sudah menjadi member belum cukup lama, sering menggunakan maskapai dan dengan jarak yang jauh sehingga sangat berpotensi untuk menghasilkan revenue. Kelompok ini juga memiliki tingkat recency yang rendah, artinya rentang waktu untuk melakukan setiap penerbangan tidak terlalu jauh.

2. Cluster 1 - **Price Sensitive**
    - Kelompok yang memiliki tingkat diskon tertinggi, tetapi dengan frekuensi yang paling rendah. Artinya kelompok ini loyalitas dan pembelianya/ penerbanganya hanya tergantung pada harga tiket yang mereka beli. Maka kelompok ini mudah dipengaruhi melalui diskon dan penawaran

3. Cluster 2 - **Can't Lose Them**
    - Kelompok yang menjadi member belum terlalu lama tetapi memiliki aktivitas dan penggunaan maskapai yang hampir sama dengan kelompok yang sudah menjadi member sudah cukup lama. Kelompok ini juga memiliki tingkat diskon yang didapatkan rendah. Artinya kelompok ini berpotensi menggunakan maskapai secara konsisten dan berkelanjutan

4. Cluster 3 - **Loyal Customers**
    - Kelompok customer yang telah menjadi member dalam jangka waktu paling lama dan memiliki aktivitas penerbangan sedang, rentang waktu untuk melakukan penerbangan tidak terlalu jauh dan cukup sering menggunakan maskapai.
"""

